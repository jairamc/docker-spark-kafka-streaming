{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acceleration Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, Minutes, StreamingContext}\n",
    "import org.apache.kafka.common.serialization.{BytesDeserializer, StringDeserializer}\n",
    "import org.apache.spark.streaming.kafka010.KafkaUtils\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "import com.fasterxml.jackson.databind.ObjectMapper\n",
    "import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
    "import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import collection.JavaConverters.mapAsJavaMapConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(1))\n",
    "ssc.remember(Minutes(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Kafka input stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val consumerParams = Map[String, Object](\n",
    "  \"bootstrap.servers\" -> \"kafka:9092\",\n",
    "  \"key.deserializer\" -> classOf[BytesDeserializer],\n",
    "  \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"group.id\" -> \"spark-notebook\",\n",
    "  \"auto.offset.reset\" -> \"earliest\",\n",
    "  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    ")\n",
    "\n",
    "val topics = Array(\"android\")\n",
    "val stream = KafkaUtils.createDirectStream[String, String](\n",
    "  ssc,\n",
    "  PreferConsistent,\n",
    "  Subscribe[String, String](topics, consumerParams)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Input Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val schema = new StructType().add(\"x\", \"float\").add(\"y\", \"float\").add(\"z\", \"float\").add(\"timestamp\", \"long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stream.foreachRDD { rdd =>\n",
    "  spark.read.schema(schema).json(rdd.map(_.value())).createOrReplaceTempView(\"locations\")\n",
    "  spark.sql(\"select avg(x) as x, avg(y) as y, avg(z) as z, min(timestamp) as timestamp from locations\").toJSON.foreachPartition {\n",
    "    partition =>\n",
    "\n",
    "      val producerParams = Map[String, Object](\n",
    "        ProducerConfig.BOOTSTRAP_SERVERS_CONFIG -> \"kafka:9092\",\n",
    "        ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringSerializer\",\n",
    "        ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringSerializer\"\n",
    "      )\n",
    " \n",
    "      val producer = new KafkaProducer[String, String](producerParams.asJava)\n",
    "      \n",
    "      partition.foreach { s =>\n",
    "        if (s != \"{}\")\n",
    "            producer.send(new ProducerRecord[String, String](\"acceleration\", s))\n",
    "      }\n",
    "      \n",
    "      producer.close()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see what we really read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+------------+-----------+---------+-------------+\n",
       "|           x|          y|        z|    timestamp|\n",
       "+------------+-----------+---------+-------------+\n",
       "|  -0.0309906|-0.20075989| 9.753952|1480415975382|\n",
       "| -0.06072998| -0.1966095| 9.986816|1480415975446|\n",
       "|-0.028884888|-0.18727112| 10.02356|1480415975510|\n",
       "|-0.042907715|-0.19052124|9.9123535|1480415975575|\n",
       "|-0.024627686|-0.20962524| 9.925446|1480415975639|\n",
       "|-0.021408081|-0.20065308| 9.945801|1480415975703|\n",
       "|-0.026153564| -0.1612091| 9.930786|1480415975767|\n",
       "|-0.062347412|-0.17887878| 9.888138|1480415975832|\n",
       "| -0.04194641|-0.18881226| 9.864151|1480415975896|\n",
       "|-0.072143555|-0.20272827|  9.95253|1480415975964|\n",
       "+------------+-----------+---------+-------------+\n",
       "only showing top 10 rows\n",
       "\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SQL\n",
    "select * from locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Verify the contents in Kafka using the console consumer\n",
    "\n",
    "The following command line tools can help print the contents to the console.\n",
    "```sh\n",
    "./bin/kafka-console-consumer.sh --topic acceleration --bootstrap-server localhost:9092\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

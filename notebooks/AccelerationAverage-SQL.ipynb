{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acceleration Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, Minutes, StreamingContext}\n",
    "import org.apache.kafka.common.serialization.{BytesDeserializer, StringDeserializer}\n",
    "import org.apache.spark.streaming.kafka010.KafkaUtils\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "import com.fasterxml.jackson.databind.ObjectMapper\n",
    "import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
    "import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import collection.JavaConverters.mapAsJavaMapConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(1))\n",
    "ssc.remember(Minutes(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val consumerParams = Map[String, Object](\n",
    "  \"bootstrap.servers\" -> \"kafka:9092\",\n",
    "  \"key.deserializer\" -> classOf[BytesDeserializer],\n",
    "  \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"group.id\" -> \"spark-notebook\",\n",
    "  \"auto.offset.reset\" -> \"earliest\",\n",
    "  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    ")\n",
    "\n",
    "val topics = Array(\"android\")\n",
    "val stream = KafkaUtils.createDirectStream[String, String](\n",
    "  ssc,\n",
    "  PreferConsistent,\n",
    "  Subscribe[String, String](topics, consumerParams)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalStateException\n",
       "Message: Adding new inputs, transformations, and output operations after starting a context is not supported\n",
       "StackTrace:   at org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:223)\n",
       "  at org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:65)\n",
       "  at org.apache.spark.streaming.dstream.ForEachDStream.<init>(ForEachDStream.scala:39)\n",
       "  at org.apache.spark.streaming.dstream.DStream.org$apache$spark$streaming$dstream$DStream$$foreachRDD(DStream.scala:653)\n",
       "  at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1.apply$mcV$sp(DStream.scala:627)\n",
       "  at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1.apply(DStream.scala:625)\n",
       "  at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1.apply(DStream.scala:625)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.SparkContext.withScope(SparkContext.scala:679)\n",
       "  at org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:264)\n",
       "  at org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:625)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Try\n",
    "\n",
    "stream.foreachRDD { rdd =>\n",
    "  Try {\n",
    "      spark.read.json(rdd.map(_.value())).createOrReplaceTempView(\"locations\")\n",
    "      spark.sql(\"select avg(x) as x, avg(y) as y, avg(z) as z, min(timestamp) as timestamp from locations\").toJSON.foreachPartition {\n",
    "          partition =>\n",
    "\n",
    "          val props = new java.util.HashMap[String, Object]()\n",
    "          props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka:9092\")\n",
    "          props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n",
    "            \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "          props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n",
    "            \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "          val producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "          partition.foreach { s => \n",
    "              val message = new ProducerRecord[String, String](\"acceleration\", s)\n",
    "              producer.send(message)\n",
    "          }\n",
    "\n",
    "          producer.close()\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`x`' given input columns: []; line 1 pos 95;\n",
       "'Project ['avg('x) AS x#758, 'avg('y) AS y#759, 'avg('z) AS z#760, 'min('timestamp) AS timestamp#761]\n",
       "+- 'Filter ('x > 5)\n",
       "   +- SubqueryAlias locations\n",
       "      +- LogicalRDD\n",
       "\n",
       "StackTrace: 'Project ['avg('x) AS x#758, 'avg('y) AS y#759, 'avg('z) AS z#760, 'min('timestamp) AS timestamp#761]\n",
       "+- 'Filter ('x > 5)\n",
       "   +- SubqueryAlias locations\n",
       "      +- LogicalRDD\n",
       "\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:269)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:279)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$8.apply(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:58)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n",
       "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n",
       "  at $anonfun$1.apply(<console>:42)\n",
       "  at $anonfun$1.apply(<console>:40)\n",
       "  at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)\n",
       "  at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)\n",
       "  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n",
       "  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n",
       "  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n",
       "  at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n",
       "  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n",
       "  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n",
       "  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n",
       "  at scala.util.Try$.apply(Try.scala:192)\n",
       "  at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
       "  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:247)\n",
       "  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)\n",
       "  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)\n",
       "  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:246)\n",
       "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTerminationOrTimeout(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SQL\n",
    "select * from locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
